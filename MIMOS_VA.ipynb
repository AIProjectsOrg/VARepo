{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "mmXr8gqFpW1z",
        "DXp6argdpbF5",
        "SMvWN2kLpowu",
        "gJSGZYcL0_ci",
        "T5PuqtaxrYpx",
        "2nn5K8hG_B69",
        "WmiOcf8sZuVh",
        "wUY3SoKzVOZW"
      ],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Video Analytics - YOLO Model Training**\n",
        "\n",
        "**Author:** [Yi-Jie Wong](https://www.linkedin.com/in/wongyijie/) & [Wingates Voon](https://www.linkedin.com/in/wingates-voon-5858391a0/) & [Yan-Chai Hum](https://www2.utar.edu.my/cv/index.jsp?cv=humyc&reqPageId=aboutMe)<br>\n",
        "**GitHub:** [Video Analytics](https://github.com/AIProjectsOrg/VARepo)<br>\n",
        "**Date created:** 2025/06/17<br>\n",
        "**Last modified:** 2025/07/19<br>\n",
        "**Description:** Train YOLOv8 for Smoking Detection in CCTV\n",
        "\n",
        "Pipeline Definition\n",
        "1.   **Setup Dependencies**\n",
        "2.   **Setup Dataset**\n",
        "3.   **Training on Original Dataset**\n",
        "4.   **Synthetic Dataset**\n",
        "5.   **Training with Synthetic Dataset**\n",
        "6.   **Evaluation on All Models**\n",
        "7.   **Export**"
      ],
      "metadata": {
        "id": "iUaq3AImYvEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Setup Dependencies**"
      ],
      "metadata": {
        "id": "mmXr8gqFpW1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics==8.3.168"
      ],
      "metadata": {
        "id": "YkBZkPfEpRzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow==1.2.1"
      ],
      "metadata": {
        "id": "Eb3zP45qm2OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Setup Dataset**"
      ],
      "metadata": {
        "id": "DXp6argdpbF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 1: CCTV Smoking Dataset\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"rvc5pEYx6sd3cZ8EBcDW\")\n",
        "project = rf.workspace(\"smoking-gqlqh\").project(\"smoking-cctv-detection-x4fjr\")\n",
        "version = project.version(4)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "SapgdYVSmJZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4291cd48-3b3a-4cc5-d6a1-8abb9551eea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_O-haB9Xhfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acff3c5-1fb9-494c-ebfc-3a82e719ff2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "# Dataset 2: Diverse Angle Smoking Dataset\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"rvc5pEYx6sd3cZ8EBcDW\")\n",
        "project = rf.workspace(\"smoking-gqlqh\").project(\"smoking-person-detection-h0a2x-zvlip\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Dataset = Dataset 1 + Dataset 2\n",
        "\n",
        "import shutil, os\n",
        "\n",
        "# make a copy of Dataset 1 as combined-dataset\n",
        "shutil.copytree(\"/content/Smoking-CCTV-Detection-4\", \"/content/combined-dataset\")\n",
        "\n",
        "# copy Dataset 2 to combined-dataset\n",
        "!cp -r \"/content/Smoking-Person-Detection-1/train/images\" \"/content/combined-dataset/train/\"\n",
        "!cp -r \"/content/Smoking-Person-Detection-1/train/labels\" \"/content/combined-dataset/train/\"\n",
        "\n",
        "!cp -r \"/content/Smoking-Person-Detection-1/valid/images\" \"/content/combined-dataset/valid/\"\n",
        "!cp -r \"/content/Smoking-Person-Detection-1/valid/labels\" \"/content/combined-dataset/valid/\"\n",
        "\n",
        "!cp -r \"/content/Smoking-Person-Detection-1/test/images\" \"/content/combined-dataset/test/\"\n",
        "!cp -r \"/content/Smoking-Person-Detection-1/test/labels\" \"/content/combined-dataset/test/\""
      ],
      "metadata": {
        "id": "_b3AUEQswCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e45b633-89f3-4b6a-d054-79c46ce43874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/combined-dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3338958059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# make a copy of Dataset 1 as combined-dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Smoking-CCTV-Detection-4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/combined-dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# copy Dataset 2 to combined-dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    574\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/combined-dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Training YOLOv8n**"
      ],
      "metadata": {
        "id": "SMvWN2kLpowu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRATEGY 1: Naive Training"
      ],
      "metadata": {
        "id": "alfzqlhDzjWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/combined-dataset/data.yaml\", epochs=50, imgsz=640, batch=16, val=False)"
      ],
      "metadata": {
        "id": "gN--fxbTprLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STRATEGY 2: Balanced Sampling"
      ],
      "metadata": {
        "id": "wvptwEq81CDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.data.dataset import YOLODataset\n",
        "import ultralytics.data.build as build\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class YOLOWeightedDataset(YOLODataset):\n",
        "    def __init__(self, *args, mode=\"train\", **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the WeightedDataset.\n",
        "\n",
        "        Args:\n",
        "            class_weights (list or numpy array): A list or array of weights corresponding to each class.\n",
        "        \"\"\"\n",
        "\n",
        "        super(YOLOWeightedDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.train_mode = \"train\" in self.prefix\n",
        "\n",
        "        # You can also specify weights manually instead\n",
        "        self.count_instances()\n",
        "        class_weights = np.sum(self.counts) / self.counts\n",
        "        self.agg_func = np.mean\n",
        "\n",
        "        self.class_weights = np.array(class_weights)\n",
        "        self.weights = self.calculate_weights()\n",
        "        self.probabilities = self.calculate_probabilities()\n",
        "\n",
        "    def count_instances(self):\n",
        "        \"\"\"\n",
        "        Count the number of instances per class\n",
        "\n",
        "        Returns:\n",
        "            dict: A dict containing the counts for each class.\n",
        "        \"\"\"\n",
        "        self.counts = [0 for i in range(len(self.data[\"names\"]))]\n",
        "        for label in self.labels:\n",
        "            cls = label['cls'].reshape(-1).astype(int)\n",
        "            for id in cls:\n",
        "                self.counts[id] += 1\n",
        "\n",
        "        self.counts = np.array(self.counts)\n",
        "        self.counts = np.where(self.counts == 0, 1, self.counts)\n",
        "\n",
        "    def calculate_weights(self):\n",
        "        \"\"\"\n",
        "        Calculate the aggregated weight for each label based on class weights.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of aggregated weights corresponding to each label.\n",
        "        \"\"\"\n",
        "        weights = []\n",
        "        for label in self.labels:\n",
        "            cls = label['cls'].reshape(-1).astype(int)\n",
        "\n",
        "            # Give a default weight to background class\n",
        "            if cls.size == 0:\n",
        "                weights.append(1)\n",
        "                continue\n",
        "\n",
        "            # Take mean of weights\n",
        "            # You can change this weight aggregation function to aggregate weights differently\n",
        "            # weight = np.mean(self.class_weights[cls])\n",
        "            # weight = np.max(self.class_weights[cls])\n",
        "            weight = self.agg_func(self.class_weights[cls])\n",
        "            weights.append(weight)\n",
        "        return weights\n",
        "\n",
        "    def calculate_probabilities(self):\n",
        "        \"\"\"\n",
        "        Calculate and store the sampling probabilities based on the weights.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of sampling probabilities corresponding to each label.\n",
        "        \"\"\"\n",
        "        total_weight = sum(self.weights)\n",
        "        probabilities = [w / total_weight for w in self.weights]\n",
        "        return probabilities\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Return transformed label information based on the sampled index.\n",
        "        \"\"\"\n",
        "        # Don't use for validation\n",
        "        if not self.train_mode:\n",
        "            return self.transforms(self.get_image_and_label(index))\n",
        "        else:\n",
        "            index = np.random.choice(len(self.labels), p=self.probabilities)\n",
        "            return self.transforms(self.get_image_and_label(index))\n",
        "\n",
        "# Monkey patch method\n",
        "build.YOLODataset = YOLOWeightedDataset\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/combined-dataset/data.yaml\", epochs=50, imgsz=640, batch=16, val=False)"
      ],
      "metadata": {
        "id": "_q1Tv3Dmp1Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart the runtime\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "RF1cHzdSLlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Synthetic Dataset**"
      ],
      "metadata": {
        "id": "gJSGZYcL0_ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Directory to save the .jpg files\n",
        "directory = \"backgrounds\"\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Indices to skip\n",
        "skip_indices = [2, 9, 10, 13, 15, 16, 19, 24, 32, 33, 36, 38,\n",
        "                44, 47, 49, 50, 53, 57, 60, 61, 64, 69, 70, 72,\n",
        "                73, 75, 77, 78, 90, 92, 93, 95, 96, 97]\n",
        "\n",
        "# Loop through numbers 1 to 100\n",
        "for i in range(1, 101):\n",
        "    if i in skip_indices:\n",
        "        print(f\"Skipping {i}.jpg\")\n",
        "        continue\n",
        "\n",
        "    url = f\"http://graphics.cs.cmu.edu/projects/whatMakesParis/testimgs/{i}.jpg\"\n",
        "    file_path = os.path.join(directory, f\"{i}.jpg\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {i}.jpg\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to download {i}.jpg: {e}\")\n"
      ],
      "metadata": {
        "id": "47F7S-HQqBSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "len(os.listdir(\"/content/combined-dataset/train/labels\"))"
      ],
      "metadata": {
        "id": "duQFHQDF4LON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def load_yolo_labels(label_path, img_width, img_height):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            _, x_center, y_center, width, height = map(float, parts)\n",
        "            x1 = int((x_center - width / 2) * img_width)\n",
        "            y1 = int((y_center - height / 2) * img_height)\n",
        "            x2 = int((x_center + width / 2) * img_width)\n",
        "            y2 = int((y_center + height / 2) * img_height)\n",
        "            bboxes.append((x1, y1, x2, y2))\n",
        "    return bboxes\n",
        "\n",
        "def create_mask(img_shape, bboxes):\n",
        "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
        "    for (x1, y1, x2, y2) in bboxes:\n",
        "        cv2.rectangle(mask, (x1, y1), (x2, y2), 255, -1)\n",
        "    return mask\n",
        "\n",
        "def extract_objects(img, bboxes):\n",
        "    objects = []\n",
        "    for (x1, y1, x2, y2) in bboxes:\n",
        "        obj = img[y1:y2, x1:x2]\n",
        "        objects.append(((x1, y1), obj))\n",
        "    return objects\n",
        "\n",
        "def paste_objects_on_background(bg, objects):\n",
        "    for (x, y), obj in objects:\n",
        "        h, w = obj.shape[:2]\n",
        "        if y + h <= bg.shape[0] and x + w <= bg.shape[1]:  # simple bounds check\n",
        "            bg[y:y+h, x:x+w] = obj\n",
        "    return bg\n",
        "\n",
        "def generate_synthetic_dataset(image_dir, label_dir, bg_dir):\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
        "    bg_files = [os.path.join(bg_dir, f) for f in os.listdir(bg_dir) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    for img_file in image_files:\n",
        "        image_path = os.path.join(image_dir, img_file)\n",
        "        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        if not os.path.exists(label_path):\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "        img_h, img_w = img.shape[:2]\n",
        "\n",
        "        bboxes = load_yolo_labels(label_path, img_w, img_h)\n",
        "        if not bboxes:\n",
        "            continue\n",
        "\n",
        "        mask = create_mask(img.shape, bboxes)\n",
        "        objects = extract_objects(img, bboxes)\n",
        "\n",
        "        bg_img_path = random.choice(bg_files)\n",
        "        bg = cv2.imread(bg_img_path)\n",
        "        bg = cv2.resize(bg, (img_w, img_h))\n",
        "\n",
        "        new_img = paste_objects_on_background(bg.copy(), objects)\n",
        "\n",
        "        # Save synthetic image and label in the same directories with prefix\n",
        "        synthetic_img_name = f\"synthetic_{img_file}\"\n",
        "        synthetic_label_name = f\"synthetic_{label_file}\"\n",
        "\n",
        "        cv2.imwrite(os.path.join(image_dir, synthetic_img_name), new_img)\n",
        "        with open(label_path, 'r') as lf, open(os.path.join(label_dir, synthetic_label_name), 'w') as out_lf:\n",
        "            out_lf.write(lf.read())\n",
        "\n",
        "        print(f\"Generated: {synthetic_img_name} and {synthetic_label_name}\")\n",
        "\n",
        "# Example usage\n",
        "generate_synthetic_dataset('/content/combined-dataset/train/images', '/content/combined-dataset/train/labels', '/content/backgrounds')\n"
      ],
      "metadata": {
        "id": "b-aDURq93_5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: randomly select an image and label pair from \"/content/Smoking-Person-Detection-1/train/images\" and \"/content/Smoking-Person-Detection-1/train/labels\". Then plot it out\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the directories\n",
        "image_dir = \"/content/combined-dataset/train/images\"\n",
        "label_dir = \"/content/combined-dataset/train/labels\"\n",
        "\n",
        "# Get list of image files\n",
        "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
        "\n",
        "if not image_files:\n",
        "    print(\"No image files found in the directory.\")\n",
        "else:\n",
        "    # Randomly select an image file\n",
        "    random_image_file = random.choice(image_files)\n",
        "    image_path = os.path.join(image_dir, random_image_file)\n",
        "\n",
        "    # Construct the corresponding label file path\n",
        "    label_file_name = os.path.splitext(random_image_file)[0] + \".txt\"\n",
        "    label_path = os.path.join(label_dir, label_file_name)\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Could not read image: {image_path}\")\n",
        "    else:\n",
        "        # Convert BGR to RGB for displaying with matplotlib\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get image dimensions\n",
        "        img_h, img_w, _ = image.shape\n",
        "\n",
        "        # Read the label file and draw bounding boxes\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) == 5:\n",
        "                        class_id = int(parts[0])\n",
        "                        xc, yc, bw, bh = map(float, parts[1:])\n",
        "\n",
        "                        # Convert YOLO format to pixel coordinates\n",
        "                        x1 = int((xc - bw / 2) * img_w)\n",
        "                        y1 = int((yc - bh / 2) * img_h)\n",
        "                        x2 = int((xc + bw / 2) * img_w)\n",
        "                        y2 = int((yc + bh / 2) * img_h)\n",
        "\n",
        "                        # Draw the bounding box (Green color, thickness 2)\n",
        "                        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "                        # Optionally, put class text on the box (simple example)\n",
        "                        # cv2.putText(image_rgb, str(class_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Plot the image with bounding boxes\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(image_rgb)\n",
        "        plt.title(f\"Image: {random_image_file}\\nLabel: {label_file_name}\")\n",
        "        plt.axis('off')  # Hide axes\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "wd3PRUmK2AMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Training with Synthetic Dataset**"
      ],
      "metadata": {
        "id": "T5PuqtaxrYpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/combined-dataset/data.yaml\", epochs=50, imgsz=640, batch=16, val=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZHOvsz1rUM0",
        "outputId": "08fcdb16-0c99-4bdb-c448-c60721056dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/combined-dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2309.2Â±953.7 MB/s, size: 201.9 KB)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined-dataset/train/labels... 4858 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4858/4858 [00:03<00:00, 1462.03it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/combined-dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1871.9Â±1209.0 MB/s, size: 101.9 KB)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/valid/labels.cache... 411 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/50      2.09G     0.9508      1.582      1.155         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:30<00:00,  9.81it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/50      2.11G     0.9594      1.132      1.153         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 10.89it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/50      2.11G     0.9558      1.013      1.149         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/50      2.11G     0.9492      0.952      1.141         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/50      2.11G     0.9082     0.8846      1.128         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/50      2.12G     0.8733     0.8275      1.108         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/50      2.13G     0.8594     0.7931      1.098         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.16it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/50      2.13G     0.8295     0.7598      1.082         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.37it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/50      2.15G     0.8106     0.7305      1.071         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/50      2.15G     0.8051     0.7132      1.067         45        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/50      2.15G     0.8043     0.7205      1.074         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.19it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/50      2.15G     0.7962     0.6933      1.067         65        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/50      2.15G     0.7668     0.6647      1.051         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/50      2.16G     0.7685     0.6588       1.05         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.19it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/50      2.16G     0.7516     0.6444       1.04         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/50      2.16G     0.7519     0.6302      1.037         62        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/50      2.16G     0.7424     0.6225      1.035         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.20it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/50      2.16G     0.7236     0.6016      1.027         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.34it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/50      2.16G     0.7286     0.6025      1.029         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/50      2.16G      0.714     0.5941      1.018         52        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      21/50      2.16G     0.7174     0.5848      1.026         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      22/50      2.16G     0.6902     0.5668      1.009         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      23/50      2.16G     0.7026     0.5709      1.017         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      24/50      2.16G      0.694      0.567      1.017         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      25/50      2.16G     0.6835     0.5418      1.007         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      26/50      2.16G     0.6832     0.5409      1.005         39        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.52it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      27/50      2.16G     0.6773     0.5401      1.005         43        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      28/50      2.16G      0.679     0.5311      1.003         31        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.26it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      29/50      2.16G     0.6647     0.5217     0.9978         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.26it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      30/50      2.16G     0.6556     0.5058     0.9915         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.30it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      31/50      2.16G     0.6547     0.4989     0.9886         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.47it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      32/50      2.16G      0.659      0.503     0.9903         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.24it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/50      2.16G     0.6503     0.4973     0.9857         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/50      2.16G     0.6542     0.5044     0.9922         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/50      2.16G     0.6368     0.4852     0.9821         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/50      2.16G     0.6336      0.475     0.9811         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/50      2.16G     0.6336     0.4692      0.978         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/50      2.16G     0.6241     0.4684     0.9786         67        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/50      2.16G     0.6249     0.4614     0.9762         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/50      2.16G      0.626      0.458     0.9753         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/50      2.16G     0.6478     0.4304     0.9591         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/50      2.16G     0.6397     0.4161     0.9502         27        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/50      2.16G     0.6296     0.4057     0.9476         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/50      2.16G     0.6185     0.3954     0.9424         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/50      2.16G      0.613     0.3897     0.9422         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/50      2.16G     0.6056     0.3802      0.936         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/50      2.16G     0.5955     0.3724     0.9287         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/50      2.16G     0.5952     0.3695     0.9314         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50      2.16G     0.5846     0.3639     0.9246         21        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50      2.16G     0.5806     0.3582     0.9212         24        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.848      0.758      0.817      0.528\n",
            "\n",
            "50 epochs completed in 0.380 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.848      0.758      0.817      0.528\n",
            "             cigarette        375        377      0.908      0.756      0.855      0.407\n",
            "                person        394        526      0.981      0.983       0.99      0.856\n",
            "                 smoke         33         43      0.655      0.535      0.606       0.32\n",
            "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.data.dataset import YOLODataset\n",
        "import ultralytics.data.build as build\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class YOLOWeightedDataset(YOLODataset):\n",
        "    def __init__(self, *args, mode=\"train\", **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the WeightedDataset.\n",
        "\n",
        "        Args:\n",
        "            class_weights (list or numpy array): A list or array of weights corresponding to each class.\n",
        "        \"\"\"\n",
        "\n",
        "        super(YOLOWeightedDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.train_mode = \"train\" in self.prefix\n",
        "\n",
        "        # You can also specify weights manually instead\n",
        "        self.count_instances()\n",
        "        class_weights = np.sum(self.counts) / self.counts\n",
        "        self.agg_func = np.mean\n",
        "\n",
        "        self.class_weights = np.array(class_weights)\n",
        "        self.weights = self.calculate_weights()\n",
        "        self.probabilities = self.calculate_probabilities()\n",
        "\n",
        "    def count_instances(self):\n",
        "        \"\"\"\n",
        "        Count the number of instances per class\n",
        "\n",
        "        Returns:\n",
        "            dict: A dict containing the counts for each class.\n",
        "        \"\"\"\n",
        "        self.counts = [0 for i in range(len(self.data[\"names\"]))]\n",
        "        for label in self.labels:\n",
        "            cls = label['cls'].reshape(-1).astype(int)\n",
        "            for id in cls:\n",
        "                self.counts[id] += 1\n",
        "\n",
        "        self.counts = np.array(self.counts)\n",
        "        self.counts = np.where(self.counts == 0, 1, self.counts)\n",
        "\n",
        "    def calculate_weights(self):\n",
        "        \"\"\"\n",
        "        Calculate the aggregated weight for each label based on class weights.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of aggregated weights corresponding to each label.\n",
        "        \"\"\"\n",
        "        weights = []\n",
        "        for label in self.labels:\n",
        "            cls = label['cls'].reshape(-1).astype(int)\n",
        "\n",
        "            # Give a default weight to background class\n",
        "            if cls.size == 0:\n",
        "                weights.append(1)\n",
        "                continue\n",
        "\n",
        "            # Take mean of weights\n",
        "            # You can change this weight aggregation function to aggregate weights differently\n",
        "            # weight = np.mean(self.class_weights[cls])\n",
        "            # weight = np.max(self.class_weights[cls])\n",
        "            weight = self.agg_func(self.class_weights[cls])\n",
        "            weights.append(weight)\n",
        "        return weights\n",
        "\n",
        "    def calculate_probabilities(self):\n",
        "        \"\"\"\n",
        "        Calculate and store the sampling probabilities based on the weights.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of sampling probabilities corresponding to each label.\n",
        "        \"\"\"\n",
        "        total_weight = sum(self.weights)\n",
        "        probabilities = [w / total_weight for w in self.weights]\n",
        "        return probabilities\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Return transformed label information based on the sampled index.\n",
        "        \"\"\"\n",
        "        # Don't use for validation\n",
        "        if not self.train_mode:\n",
        "            return self.transforms(self.get_image_and_label(index))\n",
        "        else:\n",
        "            index = np.random.choice(len(self.labels), p=self.probabilities)\n",
        "            return self.transforms(self.get_image_and_label(index))\n",
        "\n",
        "# Monkey patch method\n",
        "build.YOLODataset = YOLOWeightedDataset\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/combined-dataset/data.yaml\", epochs=50, imgsz=640, batch=16, val=False)"
      ],
      "metadata": {
        "id": "r0whFOKUrUp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de504d09-3e6d-41ef-cde9-b59567b09b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/combined-dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2290.0Â±1098.1 MB/s, size: 201.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/combined-dataset/train/labels.cache... 4858 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4858/4858 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 407.1Â±94.3 MB/s, size: 101.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/valid/labels.cache... 411 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50      2.11G     0.9804      1.692      1.195         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:30<00:00, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50      2.13G      0.976      1.213      1.189         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 10.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50      2.13G      0.987      1.117      1.197         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/50      2.14G     0.9745      1.037      1.189         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/50      2.15G      0.925      0.974      1.162         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/50      2.15G     0.8788     0.8912      1.135         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/50      2.15G     0.8784     0.8781      1.129         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/50      2.15G     0.8575     0.8416      1.121         57        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/50      2.15G     0.8513     0.8017      1.112         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/50      2.15G     0.8109     0.7714      1.096         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/50      2.15G       0.81     0.7624      1.097         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/50      2.15G     0.7991     0.7378      1.088         51        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/50      2.15G      0.787     0.7191      1.081         41        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/50      2.15G     0.7772     0.7096      1.077         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/50      2.15G     0.7726     0.6838      1.066         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/50      2.16G     0.7465      0.663      1.061         54        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/50      2.17G     0.7371     0.6409       1.05         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/50      2.17G     0.7365     0.6329      1.055         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/50      2.17G     0.7265     0.6289      1.047         62        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/50      2.17G     0.7257     0.6188      1.044         69        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/50      2.17G     0.7096     0.5997      1.037         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/50      2.17G     0.7146     0.6065      1.049         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/50      2.17G     0.7052     0.5896      1.032         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/50      2.17G     0.6958     0.5753      1.028         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/50      2.17G      0.695     0.5662      1.031         57        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/50      2.17G     0.6863     0.5615      1.025         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/50      2.17G     0.6743     0.5399      1.013         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/50      2.17G     0.6701     0.5524      1.016         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/50      2.17G     0.6618     0.5303      1.009         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/50      2.17G     0.6642     0.5285      1.011         53        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/50      2.17G      0.651     0.5251      1.005         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/50      2.17G      0.642     0.5117      1.004         42        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/50      2.17G     0.6335     0.5027     0.9987         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/50      2.17G      0.636     0.4997     0.9952         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/50      2.17G     0.6302     0.4901     0.9956         61        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/50      2.17G     0.6222     0.4793     0.9922         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/50      2.17G     0.6312      0.495     0.9953         55        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/50      2.17G     0.6156     0.4669     0.9845         59        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/50      2.17G     0.6064     0.4616     0.9862         33        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/50      2.17G     0.6041     0.4593     0.9807         54        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/50      2.17G     0.6159     0.4147      0.961         26        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 10.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/50      2.17G      0.616     0.3961     0.9598         28        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/50      2.17G     0.5961     0.3872     0.9492         20        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/50      2.17G      0.605      0.388     0.9529         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/50      2.17G     0.5735     0.3683     0.9367         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/50      2.17G     0.5775     0.3619     0.9336         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:27<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/50      2.17G     0.5859     0.3672      0.941         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/50      2.17G      0.561     0.3449     0.9291         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50      2.17G       0.55     0.3467     0.9217         25        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50      2.17G     0.5513     0.3388     0.9227         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 304/304 [00:26<00:00, 11.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  4.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.826       0.77      0.798      0.511\n",
            "\n",
            "50 epochs completed in 0.382 hours.\n",
            "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train4/weights/best.pt...\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:02<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.825       0.77      0.796      0.509\n",
            "             cigarette        375        377      0.903      0.743      0.832       0.39\n",
            "                person        394        526      0.981      0.987       0.99      0.853\n",
            "                 smoke         33         43       0.59      0.581      0.567      0.284\n",
            "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Evaluation on All Models**"
      ],
      "metadata": {
        "id": "2nn5K8hG_B69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "id": "nLvSndpb_gax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6b4247-6491-4796-a848-8d1534594b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1615.9Â±1070.1 MB/s, size: 73.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/valid/labels.cache... 411 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  8.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.813      0.745      0.821      0.529\n",
            "             cigarette        375        377      0.881      0.766      0.856      0.425\n",
            "                person        394        526      0.982      0.981       0.99      0.866\n",
            "                 smoke         33         43      0.576      0.488      0.619      0.296\n",
            "Speed: 0.4ms preprocess, 1.6ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val8\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2040.3Â±882.2 MB/s, size: 92.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/test/labels... 156 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:00<00:00, 1478.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/combined-dataset/test/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        156        388      0.847      0.833      0.867      0.552\n",
            "             cigarette        146        146      0.815      0.755      0.839      0.434\n",
            "                person        153        226      0.942      0.996      0.991      0.856\n",
            "                 smoke         13         16      0.785       0.75      0.772      0.365\n",
            "Speed: 0.9ms preprocess, 1.1ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val9\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train2/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "id": "Epi0ueD5_h0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025fe5a8-53ad-479d-a5e7-2d26710b4732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2046.5Â±1200.7 MB/s, size: 70.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/valid/labels.cache... 411 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.831      0.763      0.801      0.518\n",
            "             cigarette        375        377      0.868      0.745      0.826      0.415\n",
            "                person        394        526      0.979      0.986      0.991      0.865\n",
            "                 smoke         33         43      0.645      0.558      0.587      0.275\n",
            "Speed: 0.4ms preprocess, 0.8ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val10\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3152.3Â±1034.0 MB/s, size: 109.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/test/labels.cache... 156 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        156        388      0.836      0.846      0.853      0.558\n",
            "             cigarette        146        146      0.792      0.788      0.827       0.43\n",
            "                person        153        226      0.943          1      0.991      0.858\n",
            "                 smoke         13         16      0.771       0.75      0.742      0.384\n",
            "Speed: 0.9ms preprocess, 2.2ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val11\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train3/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "id": "_N0B_fMA9Rla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99c2143-55c7-4718-d72e-661a55d19c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1932.4Â±1133.7 MB/s, size: 67.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/valid/labels.cache... 411 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.847      0.757      0.824      0.554\n",
            "             cigarette        375        377      0.905      0.755      0.871      0.449\n",
            "                person        394        526       0.98      0.981      0.991      0.872\n",
            "                 smoke         33         43      0.656      0.535      0.608      0.343\n",
            "Speed: 0.3ms preprocess, 0.8ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val12\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2139.0Â±812.7 MB/s, size: 88.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/test/labels.cache... 156 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        156        388      0.898       0.82       0.89      0.577\n",
            "             cigarette        146        146      0.905      0.781      0.901      0.495\n",
            "                person        153        226      0.971      0.991      0.991      0.862\n",
            "                 smoke         13         16      0.817      0.688      0.779      0.374\n",
            "Speed: 0.8ms preprocess, 0.8ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val13\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train4/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/combined-dataset/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "id": "1IGJ2LFT9df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd4e184-f62d-40cc-a1b6-d248b3e6b823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1823.1Â±1111.7 MB/s, size: 73.0 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/valid/labels.cache... 411 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411/411 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00,  9.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        411        946      0.818       0.77      0.813      0.541\n",
            "             cigarette        375        377        0.9      0.741      0.852      0.434\n",
            "                person        394        526      0.979      0.987      0.991      0.871\n",
            "                 smoke         33         43      0.575      0.581      0.596      0.319\n",
            "Speed: 0.4ms preprocess, 0.8ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val14\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2581.6Â±728.7 MB/s, size: 110.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/combined-dataset/test/labels.cache... 156 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        156        388      0.811      0.829      0.843      0.563\n",
            "             cigarette        146        146      0.834      0.829      0.867      0.458\n",
            "                person        153        226      0.961      0.996      0.993      0.853\n",
            "                 smoke         13         16      0.639      0.664      0.671      0.377\n",
            "Speed: 1.0ms preprocess, 0.8ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val15\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6.5: Evaluate on CCTV dataset**"
      ],
      "metadata": {
        "id": "WmiOcf8sZuVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi0RcwgxZw9P",
        "outputId": "6fbbb609-ba41-4a97-d91d-dcd073402b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1896.6Â±439.5 MB/s, size: 117.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/valid/labels... 41 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<00:00, 1503.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Smoking-CCTV-Detection-4/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         41        195      0.726      0.705       0.76      0.448\n",
            "             cigarette         32         33      0.774      0.364      0.577      0.202\n",
            "                person         41        145      0.956      0.986      0.993      0.817\n",
            "                 smoke         15         17      0.449      0.765      0.711      0.324\n",
            "Speed: 0.9ms preprocess, 9.0ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val16\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2299.3Â±543.5 MB/s, size: 113.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/test/labels... 21 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 1465.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Smoking-CCTV-Detection-4/test/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         21        116      0.768      0.717      0.751      0.453\n",
            "             cigarette         15         15      0.728        0.4      0.477      0.164\n",
            "                person         21         85      0.944          1      0.994      0.821\n",
            "                 smoke         13         16      0.632       0.75      0.783      0.373\n",
            "Speed: 2.2ms preprocess, 4.3ms inference, 0.0ms loss, 12.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val17\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train2/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQG8cWFRZ2a8",
        "outputId": "d764635a-bee6-4422-b916-8129e1840b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2797.0Â±728.4 MB/s, size: 111.9 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/valid/labels.cache... 41 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         41        195      0.829      0.619      0.686       0.42\n",
            "             cigarette         32         33      0.906      0.212       0.41      0.106\n",
            "                person         41        145      0.973      0.998      0.994      0.821\n",
            "                 smoke         15         17      0.609      0.647      0.655      0.331\n",
            "Speed: 1.8ms preprocess, 1.1ms inference, 0.0ms loss, 8.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val18\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1777.7Â±795.8 MB/s, size: 108.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/test/labels.cache... 21 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         21        116      0.836      0.709      0.793      0.478\n",
            "             cigarette         15         15      0.849      0.377       0.59      0.192\n",
            "                person         21         85      0.931          1      0.995      0.829\n",
            "                 smoke         13         16      0.729       0.75      0.794      0.412\n",
            "Speed: 2.1ms preprocess, 1.9ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val19\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train3/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9aLyCgoZ2TZ",
        "outputId": "3b206ce4-42eb-4399-c5cb-0f7218a4873c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2010.0Â±793.7 MB/s, size: 114.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/valid/labels.cache... 41 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         41        195      0.667      0.849      0.803      0.483\n",
            "             cigarette         32         33      0.714      0.606      0.706      0.224\n",
            "                person         41        145      0.948          1      0.991      0.823\n",
            "                 smoke         15         17       0.34      0.941      0.711      0.402\n",
            "Speed: 1.9ms preprocess, 1.0ms inference, 0.0ms loss, 8.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val20\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1927.6Â±863.7 MB/s, size: 103.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/test/labels.cache... 21 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         21        116      0.718      0.782       0.79      0.486\n",
            "             cigarette         15         15       0.62      0.533      0.589      0.242\n",
            "                person         21         85      0.966          1      0.995      0.837\n",
            "                 smoke         13         16      0.568      0.812      0.787      0.378\n",
            "Speed: 1.8ms preprocess, 1.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val21\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best weights from the training run\n",
        "model = YOLO(\"/content/runs/detect/train4/weights/last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-4/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GNaMCtcZ2GE",
        "outputId": "bd21545e-f043-4c55-8ce8-0be3a1e0e90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2305.4Â±633.9 MB/s, size: 109.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/valid/labels.cache... 41 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         41        195      0.696      0.829      0.771      0.463\n",
            "             cigarette         32         33      0.705      0.545      0.641      0.206\n",
            "                person         41        145      0.954          1      0.992      0.833\n",
            "                 smoke         15         17      0.429      0.941       0.68      0.351\n",
            "Speed: 2.1ms preprocess, 1.0ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val22\u001b[0m\n",
            "Ultralytics 8.3.168 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2611.8Â±739.4 MB/s, size: 106.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Smoking-CCTV-Detection-4/test/labels.cache... 21 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         21        116      0.862      0.713      0.749      0.478\n",
            "             cigarette         15         15          1      0.453      0.575      0.239\n",
            "                person         21         85      0.979          1      0.995      0.814\n",
            "                 smoke         13         16      0.609      0.688      0.678      0.382\n",
            "Speed: 2.3ms preprocess, 1.8ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val23\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Export**"
      ],
      "metadata": {
        "id": "wUY3SoKzVOZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"/content/last.pt\")\n",
        "path = model.export(format='onnx', opset=12, simplify=True, dynamic=False, batch=1, imgsz=320)"
      ],
      "metadata": {
        "id": "nq8oqd9pVQvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OTmi1DRWMaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the best weights from the training run\n",
        "model = YOLO(\"last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/Smoking-Person-Detection-1/data.yaml\", split=\"val\", conf=0.10)\n",
        "results = model.val(data=\"/content/Smoking-Person-Detection-1/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "id": "1bnMzftamums"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the best weights from the training run\n",
        "model = YOLO(\"last.pt\")\n",
        "\n",
        "# Validate the model on the validation and test set\n",
        "results = model.val(data=\"/content/Smoking-CCTV-Detection-3/data.yaml\", split=\"test\", conf=0.10)"
      ],
      "metadata": {
        "id": "ytLSa6UEnvmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDgGEfcGnz2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}